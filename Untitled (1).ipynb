{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f572f694",
   "metadata": {
    "height": 606
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 8192, 32768, 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 8192, 32768, 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "assistant: The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 8192, 32768, 100k, 65536, and 32768, with promising results on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by combining retrieval and self-reflection. It trains language models to retrieve relevant information on-demand, reflect on their own output using special tokens called reflection tokens, and critique their own generated text. This framework allows language models to adaptively retrieve passages, generate text, and evaluate their own output, leading to improved performance on various tasks compared to existing models like ChatGPT and retrieval-augmented Llama2-chat.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing GPU memory cost and training time compared to full fine-tuning. It introduces shifted sparse attention (S2-Attn) and combines it with LoRA to effectively and efficiently extend the context sizes of pre-trained models. LongLoRA has shown strong empirical results on various tasks and models, demonstrating its effectiveness in handling longer context lengths while retaining the original model architectures and compatibility with existing techniques. Additionally, an improved version of LongLoRA, known as LoRA+, allows for the normalization and embedding layers to be trainable during the adaptation process, addressing performance gaps observed in adapting LLMs to longer context lengths.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by combining retrieval and self-reflection. It trains language models to retrieve relevant information on-demand, reflect on their own output using special tokens called reflection tokens, and critique their own generated text. This framework allows language models to adaptively retrieve passages, generate text, and evaluate their own output, leading to improved performance on various tasks compared to existing models like ChatGPT and retrieval-augmented Llama2-chat.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing GPU memory cost and training time compared to full fine-tuning. It introduces shifted sparse attention (S2-Attn) and combines it with LoRA to effectively and efficiently extend the context sizes of pre-trained models. LongLoRA has shown strong empirical results on various tasks and models, demonstrating its effectiveness in handling longer context lengths while retaining the original model architectures and compatibility with existing techniques. Additionally, an improved version of LongLoRA, known as LoRA+, allows for the normalization and embedding layers to be trainable during the adaptation process, addressing performance gaps observed in adapting LLMs to longer context lengths.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of large language models by combining retrieval and self-reflection. It trains language models to retrieve relevant information on-demand, reflect on their own output using special tokens called reflection tokens, and critique their own generated text. This framework allows language models to adaptively retrieve passages, generate text, and evaluate their own output, leading to improved performance on various tasks compared to existing models like ChatGPT and retrieval-augmented Llama2-chat.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing GPU memory cost and training time compared to full fine-tuning. It introduces shifted sparse attention (S2-Attn) and combines it with LoRA to effectively and efficiently extend the context sizes of pre-trained models. LongLoRA has shown strong empirical results on various tasks and models, demonstrating its effectiveness in handling longer context lengths while retaining the original model architectures and compatibility with existing techniques. Additionally, an improved version of LongLoRA, known as LoRA+, allows for the normalization and embedding layers to be trainable during the adaptation process, addressing performance gaps observed in adapting LLMs to longer context lengths.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from pathlib import Path\n",
    "from utils import get_doc_tools\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(initial_tools, llm=llm, verbose=True)\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "resp1 = agent.query(\"Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\")\n",
    "print(str(resp1))\n",
    "\n",
    "resp2 = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(resp2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e699da",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
