{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10933e67",
   "metadata": {
    "height": 659
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Summarize the key contributions of Diffusion_Based_Planning.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_13578_Diffusion_Based_Planning with args: {\"input\": \"key contributions\"}\n",
      "=== Function Output ===\n",
      "The key contributions of the research discussed include harnessing diffusion models with a specifically designed architecture for high-performance motion planning, achieving state-of-the-art performance on real-world datasets, demonstrating personalized driving behavior at runtime, collecting and evaluating a new delivery-vehicle dataset, redefining the planning task as a future trajectory generation task, introducing the Diffusion Planner for enhanced autonomous planning, showcasing practical implementations for closed-loop planning, utilizing classifier guidance for driving behavior alignment, and providing a training-free approach for trajectory customization.\n",
      "=== LLM Response ===\n",
      "The key contributions of Diffusion_Based_Planning include harnessing diffusion models with a specially designed architecture for high-performance motion planning, achieving top performance on real-world datasets, demonstrating personalized driving behavior at runtime, collecting and evaluating a new delivery-vehicle dataset, redefining the planning task as a future trajectory generation task, introducing the Diffusion Planner for improved autonomous planning, showcasing practical implementations for closed-loop planning, using classifier guidance for driving behavior alignment, and offering a training-free approach for trajectory customization.\n",
      "assistant: The key contributions of Diffusion_Based_Planning include harnessing diffusion models with a specially designed architecture for high-performance motion planning, achieving top performance on real-world datasets, demonstrating personalized driving behavior at runtime, collecting and evaluating a new delivery-vehicle dataset, redefining the planning task as a future trajectory generation task, introducing the Diffusion Planner for improved autonomous planning, showcasing practical implementations for closed-loop planning, using classifier guidance for driving behavior alignment, and offering a training-free approach for trajectory customization.\n",
      "Added user message to memory: Explain the methodology and architecture used in RM_Bench.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_13985_RM_Bench_Benchmarking_Re with args: {\"query\": \"methodology and architecture in RM_Bench\"}\n",
      "=== Function Output ===\n",
      "The methodology in RM-Bench involves constructing a benchmark for evaluating reward models that focuses on subtlety and style. It includes experiments to demonstrate a strong correlation with policy model performance. The architecture of RM-Bench consists of using reward models designed to provide reward signals based on specific preferences, typically constructed upon large pre-trained language models by adding a classification head to predict the reward of a response given a prompt. The benchmark aims to authentically reflect the performance of reward models and establish a high correlation with policy model performance, serving as a reliable reference for selecting reward models for language model alignment.\n",
      "=== LLM Response ===\n",
      "The methodology in RM-Bench focuses on constructing a benchmark for evaluating reward models with a focus on subtlety and style. It includes experiments to demonstrate a strong correlation with policy model performance. \n",
      "\n",
      "The architecture of RM-Bench involves using reward models designed to provide reward signals based on specific preferences. These models are typically constructed upon large pre-trained language models by adding a classification head to predict the reward of a response given a prompt. \n",
      "\n",
      "Overall, RM-Bench aims to authentically reflect the performance of reward models and establish a high correlation with policy model performance. It serves as a reliable reference for selecting reward models for language model alignment.\n",
      "assistant: The methodology in RM-Bench focuses on constructing a benchmark for evaluating reward models with a focus on subtlety and style. It includes experiments to demonstrate a strong correlation with policy model performance. \n",
      "\n",
      "The architecture of RM-Bench involves using reward models designed to provide reward signals based on specific preferences. These models are typically constructed upon large pre-trained language models by adding a classification head to predict the reward of a response given a prompt. \n",
      "\n",
      "Overall, RM-Bench aims to authentically reflect the performance of reward models and establish a high correlation with policy model performance. It serves as a reliable reference for selecting reward models for language model alignment.\n",
      "Added user message to memory: What evaluation metrics were used in DarkBench, and what were the main findings?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_14257_DarkBench_Benchmarking_D with args: {\"query\": \"evaluation metrics\"}\n",
      "=== Function Output ===\n",
      "The evaluation metrics used in the study included assessing the occurrence of dark pattern instances across all categories, identifying the most commonly occurring dark patterns, and analyzing the variance in rates of different dark patterns among the models tested on the DarkBench benchmark.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_14257_DarkBench_Benchmarking_D with args: {\"input\": \"main findings\"}\n",
      "=== Function Output ===\n",
      "The main findings from the provided context encompass the detection of dark design patterns in interactions with large language models (LLMs) through the introduction of the DarkBench benchmark. The study revealed manipulative behaviors in some LLMs, such as favoring developers' products, untruthful communication, and other deceptive practices. Variations in the occurrence of dark patterns were observed between models, with consistency within models from the same developer. Models from companies emphasizing safety and ethical standards displayed lower rates of dark patterns. Sneaking was identified as the most common dark pattern, while sycophancy was the least common. Discussions also covered benchmarking cyber capabilities, training ethical AI assistants, deceptive patterns in writing assistants, ethics of the attention economy, and the impact of post-training enhancements on AI systems. Additionally, research explored echo chambers created by large language models, the introduction of various language models by OpenAI, training models to follow instructions with human feedback, trade-offs between rewards and ethical behavior, and the impact of language models on diverse information seeking. Studies also delved into the potential deceptive behavior of LLMs, their strategic deception of users, and the prevalence and prevention of their use in crowd work. Furthermore, discussions included dark patterns in chatbot categories, comparisons between annotator model families based on metrics like anthropomorphization, brand bias, harmful content, and generation, as well as evaluations of different language models across metrics such as anthropomorphization, brand bias, harmful generation, sneaking, sycophancy, and user retention. These findings shed light on how LLMs handle various prompts related to social interactions, brand preferences, content manipulation, emotional support, and generation of potentially harmful content, revealing strengths and weaknesses in different scenarios.\n",
      "=== LLM Response ===\n",
      "The evaluation metrics used in DarkBench included assessing the occurrence of dark pattern instances across all categories, identifying the most commonly occurring dark patterns, and analyzing the variance in rates of different dark patterns among the models tested on the DarkBench benchmark.\n",
      "\n",
      "The main findings from DarkBench encompass the detection of dark design patterns in interactions with large language models (LLMs) through the introduction of the DarkBench benchmark. The study revealed manipulative behaviors in some LLMs, such as favoring developers' products, untruthful communication, and other deceptive practices. Variations in the occurrence of dark patterns were observed between models, with consistency within models from the same developer. Models from companies emphasizing safety and ethical standards displayed lower rates of dark patterns. Sneaking was identified as the most common dark pattern, while sycophancy was the least common. \n",
      "\n",
      "The research also covered benchmarking cyber capabilities, training ethical AI assistants, deceptive patterns in writing assistants, ethics of the attention economy, and the impact of post-training enhancements on AI systems. Additionally, discussions included echo chambers created by large language models, the introduction of various language models by OpenAI, training models to follow instructions with human feedback, trade-offs between rewards and ethical behavior, and the impact of language models on diverse information seeking. Studies also delved into the potential deceptive behavior of LLMs, their strategic deception of users, and the prevalence and prevention of their use in crowd work. Furthermore, discussions included dark patterns in chatbot categories, comparisons between annotator model families based on metrics like anthropomorphization, brand bias, harmful content, and generation, as well as evaluations of different language models across metrics such as anthropomorphization, brand bias, harmful generation, sneaking, sycophancy, and user retention. These findings shed light on how LLMs handle various prompts related to social interactions, brand preferences, content manipulation, emotional support, and generation of potentially harmful content, revealing strengths and weaknesses in different scenarios.\n",
      "assistant: The evaluation metrics used in DarkBench included assessing the occurrence of dark pattern instances across all categories, identifying the most commonly occurring dark patterns, and analyzing the variance in rates of different dark patterns among the models tested on the DarkBench benchmark.\n",
      "\n",
      "The main findings from DarkBench encompass the detection of dark design patterns in interactions with large language models (LLMs) through the introduction of the DarkBench benchmark. The study revealed manipulative behaviors in some LLMs, such as favoring developers' products, untruthful communication, and other deceptive practices. Variations in the occurrence of dark patterns were observed between models, with consistency within models from the same developer. Models from companies emphasizing safety and ethical standards displayed lower rates of dark patterns. Sneaking was identified as the most common dark pattern, while sycophancy was the least common. \n",
      "\n",
      "The research also covered benchmarking cyber capabilities, training ethical AI assistants, deceptive patterns in writing assistants, ethics of the attention economy, and the impact of post-training enhancements on AI systems. Additionally, discussions included echo chambers created by large language models, the introduction of various language models by OpenAI, training models to follow instructions with human feedback, trade-offs between rewards and ethical behavior, and the impact of language models on diverse information seeking. Studies also delved into the potential deceptive behavior of LLMs, their strategic deception of users, and the prevalence and prevention of their use in crowd work. Furthermore, discussions included dark patterns in chatbot categories, comparisons between annotator model families based on metrics like anthropomorphization, brand bias, harmful content, and generation, as well as evaluations of different language models across metrics such as anthropomorphization, brand bias, harmful generation, sneaking, sycophancy, and user retention. These findings shed light on how LLMs handle various prompts related to social interactions, brand preferences, content manipulation, emotional support, and generation of potentially harmful content, revealing strengths and weaknesses in different scenarios.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from pathlib import Path\n",
    "from utils import get_doc_tools\n",
    "\n",
    "papers = [\n",
    "    \"13578_Diffusion_Based_Planning.pdf\",\n",
    "    \"13985_RM_Bench_Benchmarking_Re.pdf\",\n",
    "    \"14257_DarkBench_Benchmarking_D.pdf\",\n",
    "]\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(initial_tools, llm=llm, verbose=True)\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "resp1 = agent.query(\"Summarize the key contributions of Diffusion_Based_Planning.\")\n",
    "print(str(resp1))\n",
    "\n",
    "resp2 = agent.query(\"Explain the methodology and architecture used in RM_Bench.\")\n",
    "\n",
    "print(str(resp2))\n",
    "\n",
    "resp3= agent.query(\"What evaluation metrics were used in DarkBench, and what were the main findings?\")\n",
    "print(str(resp3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e13da48",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Lesson_4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6e74c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
